{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6406c1c7-8ac2-4490-8424-7a4ce7bd9dcd",
   "metadata": {},
   "source": [
    "### Aryl Halide Clustering\n",
    "#### 1. Process dataset and remove highly correlated features (R2 > 0.95)\n",
    "#### 2. Compare PCA and UMAP for dimensionality reduction and clustering\n",
    "#### 3. Make optimal clusters and visualize molecules closest to cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca4a4e-cb9e-4ca1-9a78-0608d3de8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "# scikit learn!\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, fclusterdata\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# umap for dimensionality reduction\n",
    "import umap\n",
    "\n",
    "# nice plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c469bf-ea78-47c4-a875-4ce07470c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned aryl X dataset\n",
    "# We kept parameters for the low energy conformer (removing those for min/max and Bolztmann average, which are highly correlated)\n",
    "arylx = pd.read_csv('arylx.csv')\n",
    "arylx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeac8aa-6b8a-4e73-90ce-5e84ad96bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the molecular descriptors\n",
    "X = arylx.select_dtypes(include=['number'])\n",
    "\n",
    "# Feature scaling through standardization (or Z-score normalization) is an important preprocessing step \n",
    "# for many machine learning algorithms. Standardization involves rescaling the features such that they \n",
    "# have the properties of a standard normal distribution with a mean of zero and a standard deviation of one. \n",
    "\n",
    "X_scaled=pd.DataFrame(scale(X),index=X.index, columns=X.columns)\n",
    "\n",
    "# drop zero-variance features\n",
    "zero_std_cols = X_scaled.columns[X_scaled.std() == 0]\n",
    "X_scaled=X_scaled[X_scaled.columns.difference(zero_std_cols)]\n",
    "print (f\"Dropping {len(zero_std_cols)} features {zero_std_cols}\")\n",
    "\n",
    "# drop highly correlated features\n",
    "corr = X_scaled.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "X_scaled = X_scaled.drop(to_drop, axis=1)\n",
    "print (f\"Dropping {len(to_drop)} features {to_drop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406d95c-f28a-4491-940d-ae6bd2851089",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c09675-e2f8-471c-a6c1-0464eda829ba",
   "metadata": {},
   "source": [
    "#### At this point we have 22 descriptors remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b236e08-3895-4c45-962a-304fcfd851ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dimensionalities of the reduced representation to study\n",
    "# We will compare the performance of clustering based on reduced dimensionality representations of size 3 and 2 dimensions\n",
    "# Because tSNE cannot convert high dimension data, we only use size 3 and 3 dimensions\n",
    "dims = [3, 2]\n",
    "\n",
    "# dictionary to store data at different levels of dimensionality reduction\n",
    "dfs={}\n",
    "\n",
    "# UMAP\n",
    "np.random.seed(0)\n",
    "n_neighbors = int(np.sqrt(X_scaled.shape[1]))\n",
    "for dim in dims:\n",
    "    key = f\"umap{dim}\"\n",
    "    dfs[key] = pd.DataFrame(umap.UMAP(n_components=dim, n_neighbors=n_neighbors, random_state=np.random.RandomState(0)).fit_transform(X_scaled),\n",
    "    index=X_scaled.index)\n",
    "\n",
    "# PCA\n",
    "for dim in dims:\n",
    "    pc = pd.DataFrame(PCA(n_components=dim).fit_transform(X_scaled), index=X_scaled.index)\n",
    "    key = f\"pc{dim}\"\n",
    "    dfs[key] = pc.iloc[:, :dim]\n",
    "\n",
    "# tSNE\n",
    "for dim in dims:\n",
    "    tsne = pd.DataFrame(TSNE(n_components=dim, max_iter=1000, random_state=0).fit_transform(X_scaled), index=X_scaled.index)\n",
    "    key = f\"tsne{dim}\"\n",
    "    dfs[key] = tsne.iloc[:, :dim]\n",
    "\n",
    "# The user warnings are not concerning - relate to running calculations on 1 vs. several processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245b2d0-3abc-4578-90d3-20066e1b5932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the embeddings\n",
    "f, ax = plt.subplots(1, 3, figsize=(15, 5))  # , dpi=200)\n",
    "\n",
    "dfs[\"pc2\"].columns = [\"PC1\", \"PC2\"]\n",
    "dfs[\"umap2\"].rename(columns={0: \"UMAP1\", 1: \"UMAP2\"}, inplace=True)\n",
    "dfs[\"tsne2\"].columns = [\"tSNE1\", \"tSNE2\"]\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    data=dfs[\"pc2\"],\n",
    "    s=15,\n",
    "    alpha=0.5,\n",
    "    linewidth=0.25,\n",
    "    edgecolor=\"face\",\n",
    "    ax=ax[0],\n",
    ").set_title(\"PC projection\")\n",
    "sns.scatterplot(\n",
    "    x=\"UMAP1\",\n",
    "    y=\"UMAP2\",\n",
    "    data=dfs[\"umap2\"],\n",
    "    s=15,\n",
    "    alpha=0.5,\n",
    "    linewidth=0.25,\n",
    "    edgecolor=\"face\",\n",
    "    ax=ax[1],\n",
    ").set_title(\"UMAP projection\")\n",
    "sns.scatterplot(\n",
    "    x=\"tSNE1\",\n",
    "    y=\"tSNE2\",\n",
    "    data=dfs[\"tsne2\"],\n",
    "    s=15,\n",
    "    alpha=0.5,\n",
    "    linewidth=0.25,\n",
    "    edgecolor=\"face\",\n",
    "    ax=ax[2],\n",
    ").set_title(\"tSNE projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d81a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run another UMAP, compare the result with the first run\n",
    "n_neighbors = int(np.sqrt(X_scaled.shape[1]))\n",
    "print(n_neighbors)\n",
    "for dim in dims:\n",
    "    key = f\"umap_rerun_{dim}\"\n",
    "    dfs[key] = pd.DataFrame(umap.UMAP(n_components=dim, n_neighbors=n_neighbors, random_state=0).fit_transform(X_scaled),\n",
    "    index=X_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b188fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the embeddings\n",
    "f, ax = plt.subplots(1, 3, figsize=(15, 5)) #, dpi=200)\n",
    "\n",
    "dfs['pc2'].columns = ['PC1', 'PC2']\n",
    "dfs['umap2'].rename(columns={0:'UMAP1', 1:'UMAP2'}, inplace=True)\n",
    "dfs['tsne2'].columns = ['tSNE1', 'tSNE2']\n",
    "\n",
    "sns.scatterplot(x='PC1', y='PC2', data=dfs['pc2'], s=15, alpha=0.5, linewidth=0.25, edgecolor='face', ax=ax[0]).set_title(\"PC projection\")\n",
    "sns.scatterplot(x='UMAP1', y='UMAP2', data=dfs['umap2'], s=15, alpha=0.5, linewidth=0.25, edgecolor='face', ax=ax[1]).set_title(\"UMAP projection\")\n",
    "sns.scatterplot(x='tSNE1', y='tSNE2', data=dfs['tsne2'], s=15, alpha=0.5, linewidth=0.25, edgecolor='face', ax=ax[2]).set_title(\"tSNE projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d9790-8ffc-476b-a0ba-a620c4a2db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the quality of clustering approaches\n",
    "# Here we compare the effect of different dimensionality representations and the number of clusters\n",
    "# The sillhouette score quantifies the intra-cluster distance vs. inter-cluster distance.\n",
    "\n",
    "# Define the numbeor of clusters to study\n",
    "N_CLS_list = list(range(10, 30))\n",
    "\n",
    "def silhouette_scores_hierarchical(data, n_cls_list):\n",
    "    \"\"\"helper function to compute a silhouette score for hierarchical clustering using Ward linkage\"\"\"\n",
    "    z = linkage(data, method='ward')\n",
    "    result = pd.Series(index=n_cls_list, dtype=float)\n",
    "    for n_cls in n_cls_list:\n",
    "        cls = fcluster(z, n_cls, criterion='maxclust')\n",
    "        result.loc[n_cls] = silhouette_score(data, cls)\n",
    "    return result\n",
    "\n",
    "# populate silhouette scores for all number of clusters and all dimensionality reductions that are pre-calculated\n",
    "silh_scores = pd.DataFrame(index=N_CLS_list)\n",
    "\n",
    "for key, value in dfs.items():\n",
    "    silh_scores[key] = silhouette_scores_hierarchical(value, N_CLS_list)\n",
    "\n",
    "# plot the silhouette scores\n",
    "silh_scores.plot(xlabel='number of clusters',ylabel='silhouette score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the quality of clustering approaches using different scoring methods\n",
    "# DBCV ref: Moulavi, Davoud, et al. \"Density-based clustering validation.\" Proceedings of the 2014 SIAM International Conference on Data Mining. Society for Industrial and Applied Mathematics, 2014.\n",
    "# To install dbcv: python -m pip install \"git+https://github.com/FelSiq/DBCV\"\n",
    "import dbcv\n",
    "\n",
    "# Define the numbeor of clusters to study\n",
    "N_CLS_list = list(range(10, 30))\n",
    "\n",
    "def dbcv_scores_hierarchical(data, n_cls_list):\n",
    "    \"\"\"helper function to compute a dbcv score for hierarchical clustering using Ward linkage\"\"\"\n",
    "    z = linkage(data, method='ward')\n",
    "    result = pd.Series(index=n_cls_list, dtype=float)\n",
    "    for n_cls in n_cls_list:\n",
    "        cls = fcluster(z, n_cls, criterion='maxclust')\n",
    "        result.loc[n_cls] = dbcv.dbcv(data, cls)\n",
    "    return result\n",
    "\n",
    "# populate dbcv scores for all number of clusters and all dimensionality reductions that are pre-calculated\n",
    "dbcv_scores = pd.DataFrame(index=N_CLS_list)\n",
    "\n",
    "for key, value in dfs.items():\n",
    "    dbcv_scores[key] = dbcv_scores_hierarchical(value, N_CLS_list)\n",
    "\n",
    "# plot the dbcv scores\n",
    "dbcv_scores.plot(xlabel='number of clusters',ylabel='DBCV score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e58ff4-bf6b-44cb-8d36-4fa066201315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the plots above:\n",
    "# UMAP leads to better quality clusters compared to PCA (higher sillhouette scores overall)\n",
    "# For the UMAP results, using 5, 10 or 20 dimensions gives similar performance\n",
    "# for lower cluster numbers, (10-12), 2 dimensions works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07825108-8aff-468a-a000-7abddc8dbcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final number of clusters to produce (in the case of UMAP)\n",
    "NCLS = 18\n",
    "features='umap2'\n",
    "\n",
    "# linkage and clustering for selected featurization\n",
    "z = linkage(dfs[features], method=\"ward\")\n",
    "cls = fcluster(z, NCLS, criterion='maxclust')\n",
    "\n",
    "# plot clustering\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=\"UMAP1\", y=\"UMAP2\", data=dfs['umap2'], s=35, alpha=0.7, linewidth=0.25, edgecolor='face',\n",
    "palette='rainbow_r', legend='full', hue=cls).set_title(\"10 UMAPs - 2-UMAP projection\")\n",
    "_=plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271785fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final number of clusters to produce (in the case of PCA)\n",
    "NCLS = 11\n",
    "features='pc2'\n",
    "\n",
    "# linkage and clustering for selected featurization\n",
    "z = linkage(dfs[features], method=\"ward\")\n",
    "cls = fcluster(z, NCLS, criterion='maxclust')\n",
    "\n",
    "# plot clustering\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=\"PC1\", y=\"PC2\", data=dfs['pc2'], s=35, alpha=0.7, linewidth=0.25, edgecolor='face',\n",
    "palette='rainbow_r', legend='full', hue=cls).set_title(\"10 PCs - 2-PC projection\")\n",
    "_=plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final number of clusters to produce (in the case of tSNE)\n",
    "NCLS = 15\n",
    "features='tsne2'\n",
    "\n",
    "\n",
    "# linkage and clustering for selected featurization\n",
    "z = linkage(dfs[features], method=\"ward\")\n",
    "cls = fcluster(z, NCLS, criterion='maxclust')\n",
    "\n",
    "# plot clustering\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=\"tSNE1\", y=\"tSNE2\", data=dfs[features], s=35, alpha=0.7, linewidth=0.25, edgecolor='face',\n",
    "palette='rainbow_r', legend='full', hue=cls).set_title(\"10 tSNE - 2-tSNE projection\")\n",
    "_=plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ccbc5-09f6-4d23-bc32-93f08d7e6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build rdkit molecules for all candidates\n",
    "mols = pd.Series([Chem.MolFromSmiles(smi) for smi in arylx.smiles]).to_frame('mol')\n",
    "\n",
    "# How many central molecules to display?\n",
    "n_per_cluster = 5\n",
    "\n",
    "# store central candidates for\n",
    "cands=[]\n",
    "for group, data in mols.groupby(cls):\n",
    "   \n",
    "    # get descriptor data for this cluster\n",
    "    print (f\"Cluster {group}, n molecules: {len(data)}\")\n",
    "   \n",
    "    desc_data=dfs[features].loc[data.index]\n",
    "    \n",
    "    # compute distances of these molecules to their center\n",
    "    dists=pd.Series(cdist([desc_data.mean()], desc_data)[0],\n",
    "    index=desc_data.index)\n",
    "    \n",
    "    # select top n central molecules\n",
    "    selected=dists.sort_values().head(n_per_cluster).index\n",
    "    smi=mols.loc[selected]['mol'].map(Chem.MolToSmiles)\n",
    "    smi=smi.reset_index(drop=True).to_frame(f\"Cluster{group}\")\n",
    "    cands.append(smi)\n",
    "    \n",
    "    ms = data['mol'].loc[selected]\n",
    "    display(Draw.MolsToGridImage(ms, molsPerRow=n_per_cluster))\n",
    "    img = Draw.MolsToGridImage(ms, molsPerRow=n_per_cluster)\n",
    "\n",
    "    png = img.data\n",
    "    with open('./cluster_'+str(group)+'.png','wb+') as outf:\n",
    "        outf.write(png)\n",
    "cands = pd.concat(cands, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15b0512e-21cd-4037-b541-b9c48ed5083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make a handful of umaps with various random seeds\n",
    "umaps = []\n",
    "for i in range(50):\n",
    "    umaps.append(pd.DataFrame(umap.UMAP(n_components=10, n_neighbors=n_neighbors).fit_transform(X_scaled),\n",
    "                              index=X_scaled.index))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb8fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the original umap that was used for clustering\n",
    "umaps = [dfs['umap2']] + umaps\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def sample_dist(data):\n",
    "    dists = pdist(data)\n",
    "    return [ min(dists), np.mean(dists), max(dists)]\n",
    "\n",
    "def kenStone(X, k, metric='euclidean'):\n",
    "\n",
    "    # safety checks\n",
    "    assert isinstance(k, int)\n",
    "    assert k >= 2\n",
    "    assert k <= X.shape[0]\n",
    "    \n",
    "    # distance matrix\n",
    "    d = squareform(pdist(X, metric))\n",
    "    \n",
    "    # seed pick the pair that's furthest apart\n",
    "    selected = list(np.unravel_index(np.argmax(d), d.shape))\n",
    "\n",
    "    while len(selected) < k:\n",
    "        # add sample whose minimum distance to the selected samples is largest\n",
    "\n",
    "        selected.append(np.argmax(d[selected,].min(axis=0)))\n",
    "    \n",
    "    return selected\n",
    "\n",
    "# selection of dataset to use\n",
    "dat = umaps[7]\n",
    "NCLS=15\n",
    "\n",
    "ret_cls = []\n",
    "\n",
    "for dat in umaps:\n",
    "    # clustering\n",
    "    z = linkage(dat, method=\"ward\")\n",
    "    cls = fcluster(z, NCLS, criterion='maxclust')\n",
    "\n",
    "    selected = []\n",
    "    for group, d in dat.groupby(cls):\n",
    "        # compute distances of these molecules to their center\n",
    "        dists=pd.Series(cdist([d.mean()], d)[0], index=d.index)\n",
    "\n",
    "        # select top n central molecules\n",
    "        selected.append(dists.sort_values().index[0])\n",
    "        \n",
    "    dcls = sample_dist(dat.loc[selected])\n",
    "    ret_cls.append(dcls)\n",
    "\n",
    "ret_cls = pd.DataFrame(ret_cls, columns=[ 'clustering min. dist', 'clustering avg. dist', 'clustering max. dist'])\n",
    "\n",
    "ret_ks = []\n",
    "for dat in umaps:\n",
    "    ks = kenStone(dat, NCLS)\n",
    "    dks = sample_dist(dat.iloc[ks])\n",
    "    ret_ks.append(dks)\n",
    "ret_ks = pd.DataFrame(ret_ks, columns=[ 'ks min dist', 'ks avg dist', 'ks max dist'])\n",
    "\n",
    "# random sampling\n",
    "ret_rnd = []\n",
    "for dat in umaps:\n",
    "    for i in range(100):\n",
    "        ret_rnd.append(sample_dist(dat.sample(NCLS)))\n",
    "\n",
    "ret_rnd = pd.DataFrame(ret_rnd, columns=[ 'random min. dist', 'random avg. dist', 'random max. dist'])\n",
    "\n",
    "f, ax = plt.subplots(3,1, figsize=(6, 8))\n",
    "ret_cls.plot(kind='hist', histtype='step', facecolor='#008000', edgecolor='k' , fill=True,\n",
    "             subplots=True, ax=ax, color='black', bins=45, xlim=(-1, 20), density=False)\n",
    "'''ret_rnd.plot(kind='hist', histtype='step', color='#0000C0', linewidth=2, linestyle='--',\n",
    "             subplots=True, ax=ax, bins=45, xlim=(-1, 16), density=True)'''\n",
    "ax[2].legend(loc='upper left')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[2].set_ylabel('Frequency')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arylbr_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
